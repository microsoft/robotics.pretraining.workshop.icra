<!DOCTYPE HTML>
<!--
	Stellar by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<!-- Global site tag (gtag.js) - Google Analytics -->
		<script async src="https://www.googletagmanager.com/gtag/js?id=UA-48900508-9"></script>
		<script>
		  window.dataLayer = window.dataLayer || [];
		  function gtag(){dataLayer.push(arguments);}
		  gtag('js', new Date());

		  gtag('config', 'UA-48900508-9');
		</script>

		<title>Pretraining for Robotics Workshop, ICRA 2023</title>
		<meta name="description" content="Website for the Pretraining for Robotics workshop at the ICRA 2023">
		<meta name="author" content="Sai Vemprala">
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<!--[if lte IE 8]><script src="assets/js/ie/html5shiv.js"></script><![endif]-->
		<link rel="stylesheet" href="assets/css/main.css" />
		<!--[if lte IE 9]><link rel="stylesheet" href="assets/css/ie9.css" /><![endif]-->
		<!--[if lte IE 8]><link rel="stylesheet" href="assets/css/ie8.css" /><![endif]-->
		<link rel="stylesheet" href="assets/css/lightbox.css" />
	</head>
	<body>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header" class="alt">
						<!-- <span class="logo"><img src="images/bg_new.jpeg" alt="" /></span>  -->
				
						<h1><b>Pretraining for Robotics (PT4R)</b></h1>
						<h2>Workshop at the 2023 International Conference on Robotics and Automation - ICRA<br>
						London, May 29 2023, full-day workshop</h2>
					</header>

				<!-- Nav -->
					<nav id="nav">
						<ul>
							<li><a href="#about" class="active">About</a></li>
							<li><a href="#speakers">Speakers</a></li>
							<li><a href="#cfp">Call for papers</a></li>
							<li><a  style="color:red;" href="#accepted">Accepted papers</a></li>
							<li><a href="#schedule">Schedule</a></li>
							<li><a href="#organizers">Organizers</a></li>
						</ul>
					</nav>

				<!-- Main -->
					<div id="main">

						<section id="about" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>About</h2>
									</header>

									<p>
										Recent advances in machine learning have started a paradigm shift from task-specific models towards large general-purpose architectures. In the domains of language and vision we see large models such as GPT3, BERT, and CLIP that have opened avenues towards solving several applications and continue to cause an explosion of new ideas and possibilities. What does  it take to bring the same level of advancements to the field of robotics - in order to build versatile agents that can be deployed in challenging environments?

										The goal of this workshop is to analyze how we can scale robotics towards the complexity of real world by leveraging pretrained models. We will discuss how to apply the concept of large scale pretraining to robotics, so as to enable models to learn how to process diverse, multimodal perception inputs, connect perception with action, and generalize across scenarios and form factors. In particular, we are interested in analyzing the domain of pretraining for robotics from several angles such as, and not limited to:

										<ul>
										<li>  How do we build pre-trained reusable feature representations from complex inputs? </li>
										<li>  How do we learn world models that combine perception and actions? </li>
										<li>  How can we combine pretrained representations from multiple modalities such as language, vision, and geometry into robotics systems? </li>
										<li>  What are the right kinds of priors that are helpful for optimization and task planning? </li>
										<li>  How do we leverage architectures and training methods that have been successful in other domains in robotics? </li>
										<li>  How do we efficiently fine-tune pretrained models for new downstream tasks? </li>
										<li>  How best to deal with the specificities of robotics such as expensive data collection and safety constraints? </li>
										</ul>

										We hope to connect researchers from the communities of deep learning, representation learning, classical robotics, and to induce collaborations in this exciting new domain, while providing a platform to discuss recent developments, challenges and tradeoffs. 

										
									</p>

								</div>
							</div>
						</section>

						<section id="speakers" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>Speakers and panelists</h2>
									</header>
									<!-- <h3>Invited talks</h3> -->
									<ul class="features">
									   <li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;" src="images/dieterfox.jpeg" alt=""/>
										   <h3><a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></br>University of Washington / NVIDIA</h3>
									   </li>
										<li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;" src="images/macschwager.png" alt=""/>
											<h3><a href="https://web.stanford.edu/~schwager/">Mac Schwager</a></br>Stanford University</h3>
										</li>
										<li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"src="images/dorsasadigh.jpeg" alt=""/>
											<h3><a href="https://dorsa.fyi">Dorsa Sadigh</a></br>Stanford University</h3>
										</li>
									</ul>
									<ul class="features"> 
										<li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;"src="images/sanjiban.png" alt=""/>
											<h3><a href="https://www.sanjibanchoudhury.com">Sanjiban Choudhury</a></br>Cornell University</h3>
										</li>	
										<li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;" src="images/ashishkapoor.jpeg" alt=""/>
											<h3><a href="https://www.microsoft.com/en-us/research/people/akapoor/" target="_blank">Ashish Kapoor</a></br>Microsoft</h3>
										</li>	
										<li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;" src="images/kristengrauman.jpeg" alt=""/>
											<h3><a href="https://www.cs.utexas.edu/users/grauman/" target="_blank">Kristen Grauman</a></br>University of Texas Austin</h3>
										</li>
									</ul>
									<ul class="features">
										<li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;" src="images/jitendra.jpg" alt=""/>
											<h3><a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">Jitendra Malik</a></br>UC Berkeley</h3>
										</li>
										<li>
											<img style="width:9.1em; height: 9.1em; border-radius: 50%; object-fit: cover;" src="images/yukezhu.jpeg" alt=""/>
											<h3><a href="https://www.cs.utexas.edu/~yukez/" target="_blank">Yuke Zhu</a></br>University of Texas Austin</h3>
										</li>
									</ul>
								</div>
							</div>
						</section>

						<!-- make a section with the schedule for the workshop -->
						<section id="schedule" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>Schedule</h2>
									</header>

									<h3>May 29th 2023</h3>
									<ul>
										<li>8:30am - 8:50am: Breakfast</li>
										<li>8:50am - 9:00am: Introduction and opening remarks</li>
										<li>9:00am - 9:30am: <a href="https://www.cs.utexas.edu/~yukez/" target="_blank">Yuke Zhu</a></li>
										<li>9:30am - 10:00am: <a href="https://www.sanjibanchoudhury.com">Sanjiban Choudhury</a></li>
										<li>10:00am - 10:20am: Poster lightning talks (20 x 1min talk each) </li>
										<li>10:20am - 11:00am: Coffee break and poster session I</li>
										<li>11:00am - 11:30pm: <a href="https://people.eecs.berkeley.edu/~malik/" target="_blank">Jitendra Malik</a></li>
										<li>11:30am - 12:00pm: <a href="https://www.microsoft.com/en-us/research/people/akapoor/" target="_blank">Ashish Kapoor</a></li>
										<li>12:00pm - 1:00pm: Lunch break</li>
										<li>1:00pm - 1:30pm: <a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></li>
										<li>1:30pm - 2:00pm: <a href="https://www.cs.utexas.edu/users/grauman/" target="_blank">Kristen Grauman</a></li>
										<li>2:00pm - 2:30pm: Spotlight talks (4x 5min talk and 2min Q&A each)</li>
										<li>2:30pm - 3:15pm: Coffee break and poster session II</li>
										<li>3:15pm - 3:45pm: <a href="https://web.stanford.edu/~schwager/">Mac Schwager</a></li>
										<li>3:45pm - 4:15pm: <a href="https://dorsa.fyi">Dorsa Sadigh</a></li>
										<li>4:15pm - 5:00pm: Panel discussion</li>
										<li>5:00pm - 5:05pm: Closing remarks</li>
									</ul>
								</div>
							</div>
						</section>


						<section id="cfp" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>Call for papers</h2>
									</header>

									<h3>Important dates (all times AoE)</h3>
									<ul>
										<li>Submissions open: Feb 15th 2023</li>
										<li>Submission deadline: Apr 14th 2023</li>
										<li>Decision notification: Apr 30th 2023</li>
										<li>Camera ready deadline: May 14th 2023</li>
										<li>Workshop: May 29th 2023</li>
									</ul>

									<h3>Call for papers</h3>

									<p> Submission link: <a href="https://openreview.net/group?id=ICRA.org/2023/Workshop/Pretraining4Robotics">https://openreview.net/group?id=ICRA.org/2023/Workshop/Pretraining4Robotics</a></p>

									<p>In this workshop, we aim to bring together machine learning and robotics researchers who work at the intersection of these fields.
										We invite researchers to submit work in the following or related areas (non-exhaustive list):</p>
									<ul>
										<li>Multi-modal pretrained models (images, text, depth, point clouds, action information)</li>
										<li>Pretraining for perception and control</li>
										<li>How can pretraining take advantage of both perception and action?</li>
										<li>How can pretraining be useful to robots with different form factors, latencies, and distinct time and physical scales?</li>
										<li>Large dataset collection and data management techniques for robot pretraining</li>
										<li>Pretraining with simulation vs real-world data</li>
										<li>Theoretical guarantees and performance bounds for pretraining</li>
										<li>How much supervision is required? - learning from labaled vs unlabeled data</li>
										<li>What will robotics architectures look like in 10 years? Which components should or should not be pretrained?</li>
										<li>How much finetuning do pretrained models need?</li>
										<li>What can we pretraing? - Skills discovery, perception representations, perception-action loops, etc</li>
										<li>Human bottleneck: how to pretrain when humans are involved in the decision-making process?</li>
										<li>Any other related topics we might have forgotten in the list above &#128516;</li>
									</ul>

									<h4>Accepted Talks and Posters</h4>
									<p>Accepted papers will be presented in the form of posters (with lightning talks) or spotlight talks at the workshop. We encourage submissions of work in progress, as well as work that is not yet published. </p>

									<h3>Submission instructions</h3>

									<ul>
										<li>Submissions should be short papers up to 4 pages in PDF format (not counting references and an optional appendix, which can go over the limit) </li>
										<li>This workshop will not provide formal official proceedings and the papers will be available on the workshop website.</li>
									</ul>
									
								</div>
							</div>
						</section>

						<section id="accepted" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2 style="color:red;">Accepted Papers</h2>
									</header>

									<h3>Spotlight talks (top 15%)</h3>
									<ul>
										<li><a href="https://openreview.net/pdf?id=MhTRXNv7Pc">Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?</a>
											<br/> <p class="authors">Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, Yecheng Jason Ma, Claire Chen, Sneha Silwal, Aryan Jain, Vincent-Pierre Berges, Pieter Abbeel, Jitendra Malik, Dhruv Batra, Yixin Lin, Oleksandr Maksymets, Aravind Rajeswaran, Franziska Meier</p></li>
										<li><a href="https://openreview.net/pdf?id=1BrsVITE4PD">ViSaRL: Visual Reinforcement Learning Guided by Human Saliency</a>
											<br/> <p class="authors">Anthony Liang, Jesse Thomason, Erdem Biyik</p></li>
										<li><a href="https://openreview.net/pdf?id=PjYUlfpLJE">Building Long-term Spatial Temporal Semantic Map</a>
											<br/> <p class="authors">Ifrah Idrees, Trevor Wiedmann, Huda Abdulrasool, George Konidaris, Stefanie Tellex</p></li>
										<li><a href="https://openreview.net/pdf?id=AZJOXghs2yh">Zero-Shot Robot Manipulation from Passive Human Videos</a>
											<br/> <p class="authors">Homanga Bharadhwaj, Abhinav Gupta, Shubham Tulsiani, Vikash Kumar</p></li>
									</ul>

									<h3>Lightning talks</h3>
									<ul>
										<li><a href="https://openreview.net/pdf?id=HI12AzV3ZA">Road Barlow Twins: Redundancy Reduction for Motion Prediction</a>
											<br/> <p class="authors">Royden Wagner, Marvin Klemp, Carlos Fernandez Lopez, Omer Sahin Tas</p></li>
										
										<li><a href="https://openreview.net/pdf?id=zEyavwx3qf">ConceptFusion: Open-set Multimodal 3D Mapping</a>
											<br/> <p class="authors">Krishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Alaa Maalouf, Shuang Li, Ganesh Subramanian Iyer, Nikhil Varma Keetha, Ayush Tewari, Joshua B. Tenenbaum, Celso M de Melo, Madhava Krishna, Liam Paull, Florian Shkurti, Antonio Torralba</p></li>

										<li><a href="https://openreview.net/pdf?id=lRBc_5mGBQ">CLIP-Fields: Weakly Supervised Semantic Fields for Robotic Memory</a>
											<br/> <p class="authors">Nur Muhammad Mahi Shafiullah, Chris Paxton, Lerrel Pinto, Soumith Chintala, Arthur Szlam</p></li>

										<li><a href="https://openreview.net/pdf?id=5y8mWbNcFg">Fast Traversability Estimation for Wild Visual Navigation</a>
											<br/> <p class="authors">Jonas Frey, Matias Mattamala, Nived Chebrolu, Cesar Cadena, Maurice Fallon, Marco Hutter</p></li>

										<li><a href="https://openreview.net/pdf?id=LZ4lzQ_YAo0">Pretraining Neural-Networks with Neural-Fly for Rapid Online Learning</a>
											<br/> <p class="authors">Michael O'Connell, Guanya Shi, Xichen Shi, Kamyar Azizzadenesheli, Anima Anandkumar, Yisong Yue, Soon-Jo Chung</p></li>
										
										<li><a href="https://openreview.net/pdf?id=ShKj-4wLWyv">Self-Supervised Terrain Representation Learning from Unconstrained Robot Experience</a>
											<br/> <p class="authors">Haresh Karnan, Elvin Yang, Daniel Farkash, Garrett Warnell, Joydeep Biswas, Peter Stone</p></li>

										<li><a href="https://openreview.net/pdf?id=z3eGhFTTaQ">FastRLAP: A System for Learning High-Speed Driving via Deep RL and Autonomous Practicing</a>
											<br/> <p class="authors">Dhruv Shah, Kyle Stachowicz, Arjun Bhorkar, Ilya Kostrikov, Sergey Levine</p></li>

										<li><a href="https://openreview.net/pdf?id=sxKR6zhBDH">Contrastive Language, Action, and State Pre-training for Robot Learning</a>
											<br/> <p class="authors">Krishan Rana, Andrew Melnik, Niko Suenderhauf</p></li>

										<li><a href="https://openreview.net/pdf?id=SUIOuV2y-Ce">Learning from synthetic data generated with GRADE</a>
											<br/> <p class="authors">Elia Bonetto, Chenghao Xu, Aamir Ahmad</p></li>
										
										<li><a href="https://openreview.net/pdf?id=zZVEvf_mT6">Fine-Grained Object Detection and Manipulation with Segmentation-Conditioned Perceiver-Actor</a>
											<br/> <p class="authors">Shogo Akiyama, Dan Ogawa Lillrank, Kai Arulkumaran</p></li>

										<li><a href="https://openreview.net/pdf?id=qm62NWMxHV">CLIPGraphs: Multimodal Graph Networks to Infer Object-Room Affinities</a>
											<br/> <p class="authors">Ayush Agrawal, Raghav Arora, Ahana Datta, Snehasis Banerjee, Brojeshwar Bhowmick, Krishna Murthy Jatavallabhula, Mohan Sridharan, Madhava Krishna</p></li>

										<li><a href="https://openreview.net/pdf?id=M1yTyG5P7Cl">Text2Motion: From Natural Language Instructions to Feasible Plans</a>
											<br/> <p class="authors">Kevin Lin, Christopher Agia, Toki Migimatsu, Marco Pavone, Jeannette Bohg</p></li>

										<li><a href="https://openreview.net/pdf?id=fwKUOuFeH7">Masked Trajectory Models for Prediction, Representation, and Control</a>
											<br/> <p class="authors">Philipp Wu, Arjun Majumdar, Kevin Stone, Yixin Lin, Igor Mordatch, Pieter Abbeel, Aravind Rajeswaran</p></li>
										
										<li><a href="https://openreview.net/pdf?id=7c0WHyETaHC">FLIP-TD: Free Lunch Inpainting on Top-Down Images for Robotic Tasks</a>
											<br/> <p class="authors">Anukriti Singh, Vishnu Dutt Sharma, Pratap Tokekar</p></li>

										<li><a href="https://openreview.net/pdf?id=Hqb3t4Jqrk">Self-Supervised 3D Representation Learning for Robotics</a>
											<br/> <p class="authors">Ishika Singh, Anthony Liang, Mohit Shridhar, Jesse Thomason</p></li>

										<li><a href="https://openreview.net/pdf?id=2vHLRXkpcS">Improved Zero-Shot Object Localization using Contextualized Prompts and Objects in Context</a>
											<br/> <p class="authors">Gertjan J. Burghouts, Wouter Meijer, Fieke Hillerstr√∂m, Jelle van Mil, Michael van Bekkum, Marianne Schaaphok, Frank Ruis</p></li>

										<li><a href="https://openreview.net/pdf?id=uwtBulcTYs">Digital Twin of a Multi-Arm Robot Platform based on Isaac Sim for Synthetic Data Generation</a>
											<br/> <p class="authors">Juan Jose Quiroz Omana, Murilo Marques Marinho, Kanako Harada</p></li>
										
										<li><a href="https://openreview.net/pdf?id=Lx7trKCpn6a">Grounding Pretrained Features in 3D Representations</a>
											<br/> <p class="authors">Kenneth Tor Blomqvist, Francesco Milano, Jen Jen Chung, Lionel Ott, Roland Siegwart</p></li>

										<li><a href="https://openreview.net/pdf?id=7Y1pnhFJUT">TartanDrive 1.5: Improving Large Multimodal Robotics Dataset Collection and Distribution</a>
											<br/> <p class="authors">Matthew Sivaprakasam, Samuel Triest, Mateo Guaman Castro, Micah Nye, Mukhtar Maulimov, Cherie Ho, Parv Maheshwari, Wenshan Wang, Sebastian Scherer</p></li>

										<li><a href="https://openreview.net/pdf?id=SVedNXPNyq">Wait, That Feels Familiar: Learning to Extrapolate Human Preferences for Preference-Aligned Path Planning</a>
											<br/> <p class="authors">Elvin Yang, Haresh Karnan, Garrett Warnell, Peter Stone, Joydeep Biswas</p></li>
										
									</ul>

								</div>
							</div>
						</section>


						<section id="organizers" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>Organizers</h2>
									</header>

									<ul class="features">
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;" src="images/rb.jpeg" alt=""/>
											<h3><a href="http://rogeriobonatti.com/">Rogerio Bonatti</a></br>Microsoft</h3>
										</li>
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;" src="images/sv.png" alt=""/>
											<h3><a href="https://www.saihv.com">Sai Vemprala</a></br>Microsoft</h3>
										</li>
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"  src="images/mm.jpeg	" alt=""/>
											<h3><a href="https://mustafamukadam.com/">Mustafa Mukadam</a></br>Facebook AI Research</h3>
										</li>
									</ul>
									<ul class="features">
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;" src="images/lf.jpeg" alt=""/>
											<h3><a href="https://www.mirmi.tum.de/mirmi/team/figueredo-luis/">Luis Figueredo</a></br>Technical University of Munich</h3>
										</li>
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;" src="images/al.jpeg" alt=""/>
											<h3><a href="https://antonilo.github.io/">Antonio Loquercio</a></br>University of California Berkeley</h3>
										</li>
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;" src="images/xl.png" alt=""/>
											<h3><a href="https://xingyul.github.io/">Xingyu Liu</a></br>Carnegie Mellon University</h3>
										</li>
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;"  src="images/vb.jpeg" alt=""/>
											<h3><a href="https://cs.cornell.edu/~valts/">Valts Blukis</a></br>NVIDIA</h3>
										</li>
										<li>
											<img style="width:7.1em; height: 7.1em; border-radius: 50%; object-fit: cover;" src="images/raven.jpeg" alt=""/>
											<h3><a href="https://qingh097.github.io/">Huang (Raven) Huang</a></br>UC Berkeley</h3>
										</li>
									</ul>
								</div>
							</div>
						</section>

						<section id="contact" class="main">
							<div class="spotlight">
								<div class="content">
									<header class="major">
										<h2>Contact</h2>
									</header>
									<p>For questions and comments, please <a href="mailto:pretraining-for-robotics-icra-workshop@googlegroups.com">contact us</a>.</p>
								</div>
							</div>
						</section>

					</div>

				<!-- Footer -->
					<footer id="footer">
						<!-- <p class="copyright">Background image: <a href="https://www.spacetelescope.org/images/potw1712a/">NGC 3447 from Hubble WFC3</a></p> -->
						<p class="copyright">Copyright &copy; Sai Vemprala. Design: <a href="https://html5up.net">HTML5 UP</a>.</br>Design inspired by <a href="http://bayesiandeeplearning.org/">http://bayesiandeeplearning.org/</a> by Yarin Gal.</p>
					</footer>

			</div>

		<!-- Scripts  -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/skel.min.js"></script>
			<script src="assets/js/util.js"></script>
			<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
			<script src="assets/js/main.js"></script>
			<!-- Default Statcounter code for
			ml4physicalsciences.github.io
			https://ml4physicalsciences.github.io/ -->
			<script type="text/javascript">
			var sc_project=12052169;
			var sc_invisible=1;
			var sc_security="0155c36a";
			var sc_https=1;
			</script>
			<script type="text/javascript"
			src="https://www.statcounter.com/counter/counter.js"
			async></script>
			<noscript><div class="statcounter"><a title="free hit
			counter" href="https://statcounter.com/"
			target="_blank"><img class="statcounter"
			src="https://c.statcounter.com/12052169/0/0155c36a/1/"
			alt="free hit counter"></a></div></noscript>
			<!-- End of Statcounter Code -->
			<script src="assets/js/lightbox.js"></script>
	</body>
</html>
